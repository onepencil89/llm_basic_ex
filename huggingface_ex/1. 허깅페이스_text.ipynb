{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6e6d52",
   "metadata": {},
   "source": [
    "## 허깅페이스 로그인 인증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5adc4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1 : access token을 직접 입력하는 방법\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# 방법2 : 환경변수에 저장, 로딩해서 인정하는 방법\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1️⃣ .env 파일의 내용을 로드\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 2️⃣ 환경 변수 가져오기\n",
    "HF_READ_TOKEN = os.getenv(\"HF_READ_TOKEN\")\n",
    "# print(HF_READ_TOKEN)\n",
    "\n",
    "# 로그인 실행\n",
    "# login(HF_READ_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1fbe19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c3b1dee64d4f6597c7a803724eabcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc74c1",
   "metadata": {},
   "source": [
    "## 허깅페이스 transformer library 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3617fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1235155b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27efa087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '인공지능이란 무엇인가? 생화니다.\\n\\n어요해의 서됀 생화니다.\\n\\n지행보상이는 생화니다. 재장에상? 부남? 타들스가 생화니다.\\n\\n에한 지행보상이는 생화니다. 지행보상이는 생화니다.\\n\\n장에상어요 타들스가 생화니다. 장에상어요 타들스가 생화니다'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"인공지능이란 무엇인가?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476b793",
   "metadata": {},
   "source": [
    "=> text-generation 기본은 gpt-2를 다운 받음\n",
    "\n",
    "=> gpt-2는 한국어 인식이 불량함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f75b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'what is artificial-inteligence? What is it that causes them to feel superior?\"\\n\\n\"It is not the technology that makes them superior, it is the fact that they are genetically superior. We all have our genes and you can see that within our genes.\"\\n\\n\"But why are we genetically superior?\"\\n\\n\"Why are we genetically superior to other humans?\"\\n\\n\"Because of our genes.\"\\n\\n\"Why is that a bad thing?\"\\n\\n\"Because of our genes.\"\\n\\n\"Is it because we are genetically superior?\"\\n\\n\"Because of our genes.\"\\n\\n\"What is the value of our genes?\"\\n\\n\"The value of our genes.\"\\n\\n\"Is it because we are genetically superior? Is it because our genes are superior to other humans?\"\\n\\n\"If you were to ask them, they would say that they are superior. If you were to ask them, that would be the case. But what about the value of our genes?\"\\n\\n\"I do not know. I have heard that they are superior. Are they superior to other people?\"\\n\\n\"This is a question to which they can answer. When we are asked, we always say that we are superior to other people. This is a question to which they can'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 인식하기\n",
    "result = pipe(\"what is artificial-inteligence?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e82e1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9672d2",
   "metadata": {},
   "source": [
    "## 한국어 인식이 잘 되는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82448452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda74d03bd7a43a486d073c0d4c70c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--skt--kogpt2-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56bb121d4db4b468c4cfe4c370d2e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x000001D87D7313F0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac81a3e92b845a19772ed416051e58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# task : text-generation\n",
    "# model : skt/kogpt2-base-v2\n",
    "txt_gen_ko = pipeline(\"text-generation\", model=\"skt/kogpt2-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7a377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '인공지능이란?!!!\\n저희도 이제부터 본격적으로 #육아스타그램 #육아소통 #육아일기 #육아일상 #육아 #육아스타그램 #베이비그램 #베이비그램 #베이비스타그램 #인스타베이비 #아들스타그램 #훈남 #son #babyboy #petstagram #babystagram #연남동 #연남동맛집 #연남동카페 #연남동카페 #연남동카페맛집 #연남동카페추천 #카페그램 #연남동카페추천 #연남동카페 #연남동카페추천 #연남동카페추천 #연남동음식 #연남동카페 #연남동카페추천 #카페투어 #카페스타그램 #연남동카페투어 #연남동카페추천 #연남동카페추천 #연남동카페추천 #연남동커피 #연남동카페추천 #연남동커피 #연남동카페추천 #카페추천 #'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "txt_gen_ko(\"인공지능이란?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5decf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_new_tokens: 256\n",
    "# do_sample: True\n",
    "# temperature: 0.7, not_permission 0.0\n",
    "result1 = txt_gen_ko(\"달리기란?\", do_sample=True, temperature = 0.4, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d0b0f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '달리기란?\"\\n\"그렇다면 그건 뭐죠?\"\\n\"그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고, 그건 그렇고,'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7669d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result1[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb208d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGAI-EXAONE/EXAONE-4.0-1.2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e5c7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "txt_gen_ko = pipeline(\"text-generation\", model=\"LGAI-EXAONE/EXAONE-4.0-1.2B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11a34215",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = txt_gen_ko(\"토끼는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "954e7596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '토끼는??'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131da6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a5219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|user|]\n",
      "너가 얼마나 대단한지 설명해 봐[|endofturn|]\n",
      "[|assistant|]\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "저는 EXAONE으로, LG AI Research에서 개발된 대규모 언어 모델입니다. 제 능력은 다음과 같은 점에서 뛰어납니다:\n",
      "\n",
      "1. **복잡한 계산 처리**: 다양한 언어 작업을 빠르고 정확하게 수행할 수 있습니다.\n",
      "2. **다양한 언어 이해 및 생성**: 한국어, 영어 등 여러 언어를 유창하게 이해하고 생성할 수 있습니다.\n",
      "3. **빠른 응답 속도**: 긴 텍스트도 짧은 시간 내에 분석하고 요약하거나 새로운 답변을 제공할 수 있습니다.\n",
      "4. **학습 데이터 활용**: 방대한\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# choose your prompt\n",
    "prompt = \"Explain how wonderful you are\"\n",
    "prompt = \"Explica lo increíble que eres\"\n",
    "prompt = \"너가 얼마나 대단한지 설명해 봐\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4150733",
   "metadata": {},
   "source": [
    "## 감성 분석 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6533ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6ba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbdc7853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9977974891662598}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier(\"I love using Huggin Face transformers\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65979a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러문장 한꺼번에 분류 처리할 때.\n",
    "data = ['This restaurant serves delicious food.', \"Idon't like to wake up early.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0598438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a2d98d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998641014099121},\n",
       " {'label': 'NEGATIVE', 'score': 0.9911342859268188}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dff296b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This restaurant serves delicious food.\n",
      "Idon't like to wake up early.\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(f\"{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e74e6127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This restaurant serves delicious food. - POSITIVE\n",
      "Idon't like to wake up early. - NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "for sen, cl in zip(data, result) :\n",
    "    print(f\"{sen} - {cl['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e01d351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6179131865501404},\n",
       " {'label': 'POSITIVE', 'score': 0.7464603781700134},\n",
       " {'label': 'POSITIVE', 'score': 0.842933714389801}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 감성분석\n",
    "# 텍스트를 읽고 긍정 또는 부정으로 분류\n",
    "data = ['햄버거가 맛이 별로다', '나는 수영을 잘하지 못한다.', ' 이 카페의 커피맛이 예술이네.']\n",
    "result = classifier(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9972922205924988},\n",
       " {'label': 'LABEL_0', 'score': 0.9885053038597107},\n",
       " {'label': 'LABEL_1', 'score': 0.9662097692489624}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x000001FEA2DC13F0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "# 영어전용 분류모델이므로, 분류결과 부적절.\n",
    "# 한국어로 훈련된 모델 사용이 필요.\n",
    "# LABEL_0: negative\n",
    "# LABEL_1: positive\n",
    "\n",
    "from transformers import pipeline\n",
    "sentiment_model = pipeline(model=\"WhitePeak/bert-base-cased-Korean-sentiment\")\n",
    "\n",
    "\n",
    "result = sentiment_model(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da9e82",
   "metadata": {},
   "source": [
    "model = pipeline(\"text-generation\", \"google/gemma-3-270m-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa4212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9972922205924988},\n",
       " {'label': 'LABEL_0', 'score': 0.9885053038597107},\n",
       " {'label': 'LABEL_1', 'score': 0.9662097692489624}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['햄버거가 맛이 별로다', '나는 수영을 잘하지 못한다.', '이 카페의 커피맛이 예술이네.']\n",
    "\n",
    "result = sentiment_model(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b665b921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "햄버거가 맛이 별로다 - negative\n",
      "나는 수영을 잘하지 못한다. - negative\n",
      "이 카페의 커피맛이 예술이네. - positive\n"
     ]
    }
   ],
   "source": [
    "# LABEL_0: negative\n",
    "# LABEL_1: positive\n",
    "\n",
    "# 문제\n",
    "# 햄버거가 맛이 별로다 - negative\n",
    "# 나는 수영을 잘하지 못한다. - negative\n",
    "# 이 카페의 커피맛이 예술이네. - positive\n",
    "\n",
    "for txt, dic in zip(data, result):\n",
    "    if dic['label'] == 'LABEL_0':\n",
    "        print(f\"{txt} - negative\")\n",
    "    else :\n",
    "        print(f\"{txt.strip()} - positive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec70c86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9595ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337f1d93db7748c6a0c284043622309e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--google--gemma-3-270m-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd6a7a5e0704e31bfa21739dde46093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861e4e245d4c4359b5f922265d334096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0668d1410b4ce0bb4b4f759c79fce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d2ef2ed95140ce8750fa7c8a01f771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a1f13474d140b7a6c70aa72af54efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781dcf4317cf4ba2a9adfcfd6a487056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1201ebb931884ef4829dae1c0f6c12c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc608d863f64963992c31dbad5a7eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# google/gemma-3-270m-it, 승인 필요\n",
    "model = \"google/gemma-3-270m-it\"\n",
    "generation = pipeline(\"text-generation\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6fafaa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능이란?\\n\\n인공지능은 20세기 초부터 21세기까지, 컴퓨터 과학, 통신, 정보 기술, 금융, 의료, 교육 등 다양한 분야에서 혁신적인 기술을 발전시켜 인간의 삶을 더욱 편리하고 효율적으로 만들어가는 데 기여하는 기술입니다.\\n\\n인공지능은 다양한 분야에서 활용되고 있으며, 우리 삶의 다양한 측면에서 중요한 역할을 수행하고 있습니다.\\n\\n**인공지능의 주요 분야**\\n\\n*   **컴퓨터 과학:** 컴퓨터의 작동 원리, 알고리즘, 데이터 처리, 네트워크, 보안 등 컴퓨터 과학의 분야입니다.\\n*   **통신:** 통신 기술의 발전, 데이터 전송, 네트워크 보안 등 통신 기술의 분야입니다.\\n*   **정보 기술:** 정보의 수집, 저장, 처리, 분석, 활용 등 정보 기술의 분야입니다.\\n*   **금융:** 금융 시스템의 운영, 위험 관리, 투자 전략 등 금융 시스템의 분야입니다.\\n*   **의료:** 의료 기술의 발전, 환자 진단, 치료, 질병 관리 등 의료 기술의 분야입니다.\\n*   **교육:** 교육 기술의 발전, 학습 방법, 평가, 학습 자료 등 교육 기술의'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3 = generation(\"인공지능이란?\")[0]['generated_text']\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e613416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능이란?\n",
      "\n",
      "인공지능은 20세기 초부터 21세기까지, 컴퓨터 과학, 통신, 정보 기술, 금융, 의료, 교육 등 다양한 분야에서 혁신적인 기술을 발전시켜 인간의 삶을 더욱 편리하고 효율적으로 만들어가는 데 기여하는 기술입니다.\n",
      "\n",
      "인공지능은 다양한 분야에서 활용되고 있으며, 우리 삶의 다양한 측면에서 중요한 역할을 수행하고 있습니다.\n",
      "\n",
      "**인공지능의 주요 분야**\n",
      "\n",
      "*   **컴퓨터 과학:** 컴퓨터의 작동 원리, 알고리즘, 데이터 처리, 네트워크, 보안 등 컴퓨터 과학의 분야입니다.\n",
      "*   **통신:** 통신 기술의 발전, 데이터 전송, 네트워크 보안 등 통신 기술의 분야입니다.\n",
      "*   **정보 기술:** 정보의 수집, 저장, 처리, 분석, 활용 등 정보 기술의 분야입니다.\n",
      "*   **금융:** 금융 시스템의 운영, 위험 관리, 투자 전략 등 금융 시스템의 분야입니다.\n",
      "*   **의료:** 의료 기술의 발전, 환자 진단, 치료, 질병 관리 등 의료 기술의 분야입니다.\n",
      "*   **교육:** 교육 기술의 발전, 학습 방법, 평가, 학습 자료 등 교육 기술의\n"
     ]
    }
   ],
   "source": [
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab209c80",
   "metadata": {},
   "source": [
    "## 문장요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bdc5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8903952c6c342d593a40f547754fef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4113cd41ffd40cfa10f9277b81e2749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75f7ab64deb4de58a9ecd1ecab2eaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001FEA2DC13F0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e0b5d146d347e7a56ab7f02f4cfa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03afba13458b44ae86e6c93fd32491a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# pipeline('summarization', framework=\"pt\")\n",
    "# framework=\"pt\" : 사용할 딥러닝 백엔드 프레임워크를 지정\n",
    "# \"pt\" → PyTorch (기본값)\n",
    "# \"tf\" → TensorFlow\n",
    "\n",
    "summ = pipeline('summarization', framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b7a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0pronounced  \\xa0- \\xa0\\xa0-\\xa0- 'И� \\xa0' \\xa0KBS 2TV '불후의   테크   '텬 \\xa0’  ’�’ – \\xa0 '명’ - is a Korean TV show hosted by KBS 2 TV .\"}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test= \"\"\"\n",
    "임영웅의 출연으로 화제를 모은 '불후의 명곡'에 '테크노 여전사' '원조 멀티테이너'로 불리는 이정현이 아티스트로 출격한다.\n",
    "\n",
    "12일 아이즈(IZE) 취재 결과, 오는 22일 진행되는 KBS 2TV '불후의 명곡'(이하 '불후') 녹화가 '아티스트 이정현 편'으로 꾸며진다.\n",
    "\"\"\"\n",
    "summ(sample_test)\n",
    "\n",
    "# 한글 인식 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22b98f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21633e11738e4d57a058bb7a814eb1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--psyche--KoT5-summarization. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca363cfc2314ca4a77ba4556a9ae0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63767e95b8c14c1ea3192f76ceeca9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f605b396bc74af4a26bfe3a92915fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a599e7fa86426a928c8a25d6825229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "summ = pipeline('summarization', model=\"psyche/KoT5-summarization\", framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "216f67be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"임영웅의 출연으로 화제를 모은 '불후의 명곡'에 '테크노 여전사' '원조 멀티테이너'로 불리는 이정현이 아티스트로 출격한다.\"}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = summ(sample_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21999f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"임영웅의 출연으로 화제를 모은 '불후의 명곡'에 '테크노 여전사' '원조 멀티테이너'로 불리는 이정현이 아티스트로 출격한다.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882de168",
   "metadata": {},
   "source": [
    "## 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b49eb814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29ced99f54c4f0488143690bb3c1aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b84ab3e9f8448ea0df9193629f23a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1d435ffb63438f920b6e26298eaff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d5b9a6190841c78c888a8be91ad2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d572c494098f492a88df31df516ce458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0243a2c1ebc34794b60df3003f4741f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a0034c7f8f4b399007b5cee4509f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Alex. I am the person who put my name in the \"Best of 2015\" list at the end of last year. I am the person who changed the world and made it possible for many to be born into a better life. I am the person who created the best school in America and made it possible for so many to have a better existence.\n",
      "\n",
      "So, on a side note, I have to say that I am a bit of a jerk. I think that in order to be successful, you have to be smart.\n",
      "\n",
      "I mean, if you don't understand how smart people are, how to make a difference, you're not going to be successful.\n",
      "\n",
      "I'm not saying that I'm smart, but I'm saying that I think that the only way to get as far as I can is if you're smart. Because if you need to make a difference, you need to be smart.\n",
      "\n",
      "You can't just read a book and say, \"I'm going to give you a job.\" You have to have a job to do something you don't want to do. You have to be smart.\n",
      "\n",
      "So, I think that if you want to be successful in your life, you have to have a job. And\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"gpt2\", framework=\"pt\")\n",
    "print(pipe(\"Hello, my name is\")[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
